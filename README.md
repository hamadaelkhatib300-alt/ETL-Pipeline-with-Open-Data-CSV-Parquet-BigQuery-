# ETL-Pipeline-with-Open-Data-CSV-Parquet-BigQuery-
Build a modern ETL pipeline that ingests open CSV data, cleans and transforms it using efficient tools (like Polars), stores in Parquet format, and loads into Google BigQuery for querying. 

Milestone 1: Data Collection, Exploration & Preprocessing
• Collect raw open data (e.g., weather or transportation CSV).
• Explore schemas and quality issues.
• Convert CSV to Parquet for efficient storage.
Milestone 2: Model / System Development & Evaluation
• Develop transformation workflows in Python (Polars/Pandas).
• Validate data correctness and completeness.
Milestone 3: Deployment (Real-Time or Batch)
• Deploy batch pipeline to run on schedule (cron/Airflow).
• Load Parquet data into BigQuery.
Milestone 4: MLOps / Monitoring / Automation
• Automate pipeline triggers.
• Monitor job success and data freshness.
Milestone 5: Final Documentation, Demo & Presentation
• Document ETL logic, schemas, and BigQuery tables.
• Demo queries and pipeline execution. 

